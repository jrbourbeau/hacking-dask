{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e6c3f1-9673-4eb0-81b1-3fe9ac7e84cf",
   "metadata": {},
   "source": [
    "# Advanced Collections: _Custom Operations_\n",
    "\n",
    "In the overview notabook we discussed some of the many algorithms that are pre-defined for different types of Dask collections\n",
    "(such as Arrays and DataFrames). These include operations like `mean`, `max`, `value_counts` and many other standard operations.\n",
    "\n",
    "In this notebook we'll explore how those operations are implemented and learn how  to construct our own custom operations to use with Dask Arrays and Dask Dataframes.\n",
    "\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "  - [Array Tutorial](https://tutorial.dask.org/03_array.html)\n",
    "  - [Best Practices](https://docs.dask.org/en/latest/best-practices.html#learn-techniques-for-customization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0fe241-96cf-414d-b5ee-d700b51651df",
   "metadata": {},
   "source": [
    "## Blocked Algorithms\n",
    "\n",
    "Dask computations are implemented using _blocked algorithms_. These algorithms break up a computation on a large array into many computations on smaller pieces of the array. This minimizes the memory load (amount of RAM) of computations and allows for working with larger-than-memory datasets in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca494a4-362a-4503-9725-e79ebeee84c4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "x = da.random.random(size=(1_000, 1_000), chunks=(250, 500))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c5b30-fab9-4cd8-9e7f-a2dd4bbf1f4c",
   "metadata": {},
   "source": [
    "In the overview notebook we looked at the task graph for the following computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776c7645-3c80-4390-a8ed-dde0d4463712",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "result = (x + x.T).sum(axis=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f181e4-0205-4f9a-89db-4f8f3a54d801",
   "metadata": {},
   "source": [
    "Now let's break that down a bit and look at the task graph for just one part of that computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44e637-1782-4eb1-89c3-0025230e1fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.T.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087e539-2d92-4ed4-98ab-03471de15c8d",
   "metadata": {},
   "source": [
    "This graph demonstrates how blocked algorithms work. In the perfectly parallelizable situation, Dask can operate on each block in isolation and then reassemble the results from the outputs. Dask makes it easy to contruct graphs like this using a numpy-like API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-minority",
   "metadata": {},
   "source": [
    "## Custom Block Computations\n",
    "Block computations operate on a per-block basis. So each block gets the function applied to it, and the output has the same chunk location as the input.\n",
    "\n",
    "Some examples include the following:\n",
    "- custom IO operations\n",
    "- applying embarassingly parallel functions for which there is no exising Dask implementation\n",
    "\n",
    "![map_blocks](images/custom_operations_map_blocks.png)\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "   - [`dask.array.map_blocks`](https://docs.dask.org/en/latest/array-api.html?highlight=map_blocks#dask.array.Array.map_blocks)\n",
    "   - [`dask.dataframe.map_partitions`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-albert",
   "metadata": {},
   "source": [
    "### `map_blocks`\n",
    "\n",
    "Let's imagine that there was no `da.random.random` method. We can create our own version using `map_blocks`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262fd43-b432-4e40-aced-4f9ad633d0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_sample():\n",
    "    return np.random.random(size=(250, 500))\n",
    "\n",
    "x = da.map_blocks(random_sample, chunks=((250, 250, 250, 250), (500, 500)), dtype=float)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660b3ed-494e-4c41-af0f-59b0e1626194",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-congo",
   "metadata": {},
   "source": [
    "> #### Understanding `chunks` argument\n",
    ">\n",
    "> In the example above we explicitly declare what the size of the output chunks will be ``chunks=((250, 250, 250, 250), (500, 500))`` this means 8 chunks each with shape `(250, 500)` you'll also see the chunks argument written in the short version where only the shape of one chunk is defined ``chunks=(250, 500)``. These mean the same thing.\n",
    ">\n",
    "> Specifying the output chunks is very useful when doing more involved operations with ``map_blocks``. By specifying ``chunks``, you can guarantee that the output will have the right shape which lets you properly chain together other operations. \n",
    ">\n",
    "> When in doubt, you can always find shape and chunk information in the array representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f0551-bf38-4e95-a8a4-3aa9ac5b26fd",
   "metadata": {},
   "source": [
    "In that example we created an array from scratch by passing in `dtype` and `chunks`. Next we'll consider the case of applying `map_blocks` to existing arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-devon",
   "metadata": {},
   "source": [
    "#### Multiple arrays\n",
    "\n",
    "``map_blocks`` can be used on single arrays or to combine several arrays. When multiple arrays are passed, ``map_blocks``\n",
    "aligns blocks by block location without regard to shape.\n",
    "\n",
    "In the following example we have two arrays with the same number of blocks\n",
    "but with different shape and chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = da.arange(1000, chunks=(100,))\n",
    "b = da.arange(100, chunks=(10,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-maldives",
   "metadata": {},
   "source": [
    "Let's take a look at these arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-excess",
   "metadata": {},
   "source": [
    "We can pass these arrays into ``map_blocks`` using a function that takes two inputs, calculates the max of each, than then returns a numpy array of the outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(a, b):\n",
    "    return np.array([a.max(), b.max()])\n",
    "\n",
    "result = da.map_blocks(func, a, b, chunks=(2,))\n",
    "result.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-output",
   "metadata": {},
   "source": [
    "#### Special arguments\n",
    "\n",
    "There are special arguments (``block_info`` and ``block_id``) that you can use within ``map_blocks`` functions. ``block_id`` gives the index of the block within the chunks, so for a 1D array it will be something like `(i,)`. ``block_info`` is a dictionary where there is an integer key for each input dask array and a `None` key for the output array.\n",
    "\n",
    "Let's use the example above and print ``block_info`` for the first block so that we can get a sense of what information is contained in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def func(a, b, block_id=None, block_info=None):\n",
    "    if block_id == (0,):\n",
    "        pprint(block_info)\n",
    "    return np.array([a.max(), b.max()])\n",
    "\n",
    "da.map_blocks(func, a, b, chunks=(2,)).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-kentucky",
   "metadata": {},
   "source": [
    "One of the use cases for the ``block_info`` and ``block_id`` arguments is to create an array from scratch by reading in specific files for each block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-lover",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Say you have a set of images that each represent a particular portion of a scene. How can you use the\n",
    "technique we just learned to patch them together? \n",
    "\n",
    "Let's look at what is in the puzzle directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls puzzle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-expansion",
   "metadata": {},
   "source": [
    "The following cell displays the completed puzzle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = imread(\"puzzle/bicycle.png\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-stake",
   "metadata": {},
   "source": [
    "Now use ``map_blocks`` to read in the puzzle pieces from \"bicycle_i_j.png\". Note that each image piece has 3 dimensions: x, y, and RGBA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f531c-8d58-4987-bbda-8127addfe835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that reads one file\n",
    "def reader(block, block_id=None):\n",
    "    ... = block_id  # unpack block_id to get chunk location\n",
    "    filename = ...  # use chunk location to get the correct file\n",
    "    return imread(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-daughter",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "# define a function that reads one file\n",
    "def reader(block_id=None):\n",
    "    ii, jj, _ = block_id                        # unpack block_id to get chunk location\n",
    "    filename = f\"puzzle/bicycle_{ii}_{jj}.png\"  # use chunk location to get the correct file\n",
    "    return imread(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5149566-5665-496c-b6f8-b935f8d3be0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map that function to every block and set the expected dtype and chunk pattern of the output\n",
    "result = da.map_blocks(reader, dtype=int, chunks=((24, 24), (24, 24), (4,)))\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-interim",
   "metadata": {},
   "source": [
    "### ``map_partitions``\n",
    "\n",
    "In Dask dataframe there is a similar method to ``map_blocks`` but it is called ``map_partitions``.\n",
    "\n",
    "Here is an example of using it to check if the sum of two columns is greater than some arbitrary threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01aeac1-a313-4ab4-b940-719ee5fbd116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "ddf = dask.datasets.timeseries()\n",
    "\n",
    "result = ddf.map_partitions(lambda df, threshold: (df.x + df.y) > 0, threshold=0)\n",
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-preparation",
   "metadata": {},
   "source": [
    "#### Internal uses\n",
    "In practice ``map_partitions`` is used to implement many of the helper dataframe methods\n",
    "that let Dask dataframe mimic Pandas. Here is the implementation of `ddf.index` for instance:\n",
    "\n",
    "```python\n",
    "@property\n",
    "def index(self):\n",
    "    \"\"\"Return dask Index instance\"\"\"\n",
    "    return self.map_partitions(\n",
    "        getattr,\n",
    "        \"index\",\n",
    "        token=self._name + \"-index\",\n",
    "        meta=self._meta.index,\n",
    "        enforce_metadata=False,\n",
    "    )\n",
    "```\n",
    "\n",
    "[source](https://github.com/dask/dask/blob/09862ed99a02bf3a617ac53b116f9ecf81eea338/dask/dataframe/core.py#L458-L467)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-norwegian",
   "metadata": {},
   "source": [
    "#### Understanding `meta` argument\n",
    "\n",
    "Dask dataframes and dask arrays have a special attribute called `_meta` that allows them to know metadata about the type of dataframe/array that they represent. This metadata includes:\n",
    " - dtype (int, float)\n",
    " - column names and order\n",
    " - name\n",
    " - type (pandas dataframe, cudf dataframe)\n",
    " \n",
    "**Related documentation**\n",
    "\n",
    "- [Dataframe metadata](https://docs.dask.org/en/latest/dataframe-design.html#metadata)\n",
    "\n",
    "This information is stored in an empty object of the proper type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ddf._meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-architect",
   "metadata": {},
   "source": [
    "That's how dask knows what to render when you display a dask object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-prairie",
   "metadata": {},
   "source": [
    "When you add an item to the task graph, Dask tries to run the function on the meta before you call compute. \n",
    "\n",
    "This approach has several benefits:\n",
    "\n",
    "- it gives Dask a sense of what the output will look like. \n",
    "- if there are fundamental issues, Dask will fail fast\n",
    "\n",
    "Here's a few examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3fb018-de20-4f75-9d7d-441a6ec3a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-excitement",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.name.str.startswith(\"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa14a6-fd76-4dff-89de-def4039b0103",
   "metadata": {},
   "source": [
    "See how the output looks right? The dtypes are correct, the type is a `Series` rather than a `DataFrame` like the input.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Try using `startswith` on a different column and see what you get :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344e1d3-e417-4324-82e0-57e8efab5fd4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "ddf.x.str.startswith(\"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-memory",
   "metadata": {},
   "source": [
    "### Declaring meta\n",
    "\n",
    "Sometimes running the function on a miniature version of the data doesn't produce a result that is similar enough to your expected output. \n",
    "\n",
    "In those cases you can provide a `meta` manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ddf.map_partitions(lambda df, threshold: (df.x + df.y) > threshold, threshold=0, meta=bool)\n",
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-sharing",
   "metadata": {},
   "source": [
    "### `map_overlap`\n",
    "Sometimes you want to operate on a per-block basis, but you need some information from neighboring blocks. \n",
    "\n",
    "Example operations include the following:\n",
    "\n",
    "- Convolve a filter across an image\n",
    "- Rolling sum/mean/max, …\n",
    "- Search for image motifs like a Gaussian blob that might span the border of a block\n",
    "- Evaluate a partial derivative\n",
    "\n",
    "Dask Array supports these operations by creating a new array where each block is slightly expanded by the borders of its neighbors. \n",
    "\n",
    "![](https://docs.dask.org/en/latest/_images/overlapping-neighbors.png)\n",
    "\n",
    "This costs an excess copy and the communication of many small chunks, but allows localized functions to evaluate in an embarrassingly parallel manner.\n",
    "\n",
    "**Related Documentation**\n",
    "   - [Array Overlap](https://docs.dask.org/en/latest/array-overlap.html)\n",
    "\n",
    "The main API for these computations is the ``map_overlap`` method. ``map_overlap`` is very similar to ``map_blocks`` but has the additional arguments: ``depth``, ``boundary``, and ``trim``.\n",
    "\n",
    "Here is an example of calculating the derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e78c226-a318-4772-959d-f099f8564185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "a = np.array([1, 1, 2, 3, 3, 3, 2, 1, 1])\n",
    "a = da.from_array(a, chunks=5)\n",
    "\n",
    "plt.plot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde80b97-1781-4104-8ca2-b43e4544fc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(a):\n",
    "    return a - np.roll(a, 1)\n",
    "\n",
    "b = a.map_overlap(derivative, depth=1, boundary=None)\n",
    "b.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-lying",
   "metadata": {},
   "source": [
    "In this case each block shares 1 value from its neighboring block: ``depth``. And since we set ``boundary=0``on the outer edges of the array, the first and last block are padded with the integer 0. Since we haven't specified ``trim`` it is true by default meaning that the overlap is removed before returning the results.\n",
    "\n",
    "If you inspect the task graph you'll see two mostly independent towers of tasks, with just some value sharing at the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.visualize(collapse_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-crowd",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Lets apply a gaussian filter to an image following the example from the [scipy docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.gaussian_filter.html).\n",
    "\n",
    "First create a dask array from the numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "import dask.array as da\n",
    "\n",
    "a = da.from_array(misc.ascent(), chunks=(128, 128))\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-editing",
   "metadata": {},
   "source": [
    "Now use ``map_overlap`` to apply ``gausian_filter`` to each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "b = a.map_overlap(gaussian_filter, sigma=5, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-worcester",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "b = a.map_overlap(gaussian_filter, sigma=5, depth=10, boundary=\"periodic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-anaheim",
   "metadata": {},
   "source": [
    "Check what you've come up with by plotting the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax1.imshow(a)\n",
    "ax2.imshow(b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-mistake",
   "metadata": {},
   "source": [
    "> Notice that if you set the depth to a smaller value, you can see the edges of the blocks in the output image.\n",
    "\n",
    "\n",
    "## Blockwise Computations\n",
    "\n",
    "Blockwise computations provide the infrastructure for implementing ``map_blocks``, ``reduction`` and many\n",
    "of the elementwise methods that make up the Array API. \n",
    "\n",
    "Example operations include the following:\n",
    "\n",
    "- simple mapping operations like adding two arrays\n",
    "- operations that flip the dataset like transpose\n",
    "- reductions like tensordot and inner product\n",
    "\n",
    "\n",
    "<img src=images/custom_operations_map_blocks.png width=30% /><img src=images/custom_operations_blockwise.png width=30% /><img src=images/custom_operations_reduction.png align=right width=30% />\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "   - [API Documentation](https://docs.dask.org/en/latest/array-api.html#dask.array.blockwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a276edc9-d092-4620-a542-80abd3d0e81a",
   "metadata": {},
   "source": [
    "### Internal uses\n",
    "\n",
    "This is the internal definition of transpose on dask.Array. In it you can see that there is a\n",
    "regular ``np.transpose`` applied within each block and then the blocks are themselves transposed.\n",
    "\n",
    "```python\n",
    "def transpose(a, axes=None):\n",
    "    if axes:\n",
    "        if len(axes) != a.ndim:\n",
    "            raise ValueError(\"axes don't match array\")\n",
    "    else:\n",
    "        axes = tuple(range(a.ndim))[::-1]\n",
    "    axes = tuple(d + a.ndim if d < 0 else d for d in axes)\n",
    "    return blockwise(\n",
    "        np.transpose, axes, a, tuple(range(a.ndim)), dtype=a.dtype, axes=axes\n",
    "    )\n",
    "```\n",
    "\n",
    "[source](https://github.com/dask/dask/blob/4569b150db36af0aa9d9a8d318b4239a78e2eaec/dask/array/routines.py#L161:L170)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f437fff-688f-4fa7-859e-1139750f4a76",
   "metadata": {},
   "source": [
    "We can implement our own version of transpose using `blockise` directly. If you are comfortable with matrix math then the notation will look very familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b4fe2-95c7-4434-ae41-5e124cbefd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = da.blockwise(np.transpose, \"ji\", x, \"ij\")\n",
    "result.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cd7d6d-9b90-4df6-82a5-ab90795a57f1",
   "metadata": {},
   "source": [
    "Using `blockwise` we can operate on multiple arrays by just passing more arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a54ad3d-34fd-48ca-b41f-e28b35e925a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = da.blockwise(np.add, \"ij\", x, \"ij\", x.T, \"ij\", dtype=x.dtype)\n",
    "result.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a8f5a5-a08e-488b-a309-1cf2a1ca0674",
   "metadata": {},
   "source": [
    "This is equivalent to `x + x.T`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b1be59-cff9-4fb9-b54b-72918cdd7845",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x + x.T).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5599d4de-2f1d-43db-86fc-31148c897010",
   "metadata": {},
   "source": [
    "We can increase or decrease the dimensionality of the output by changing the out index. There are lots more examples of how to use `blockwise` in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc7eba5-eff9-4a63-9b22-49e2f44e386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.blockwise?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cb0c3f-1aca-42db-9ec3-565a45c68a45",
   "metadata": {},
   "source": [
    "## Reduction\n",
    "Each dask collection has a `reduction` method. This is the generalized method that supports operations that reduce the dimensionality of the inputs.\n",
    "\n",
    "The difference between `blockwise` and `reduction` is that with `reduction` you have finer grained control over the behavior of the tree-reduce.\n",
    "\n",
    "![Custom operations: reduction](images/custom_operations_reduction.png)\n",
    "\n",
    "**Related Documentation**\n",
    "   - [`dask.array.reduction`](http://dask.pydata.org/en/latest/array-api.html#dask.dataframe.Array.reduction)\n",
    "   - [`dask.dataframe.reduction`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-representation",
   "metadata": {},
   "source": [
    "### Internal uses\n",
    "\n",
    "This is the internal definition of sum on dask.Array. In it you can see that there is a\n",
    "regular ``np.sum`` applied across each block and then tree-reduced with ``np.sum`` again.\n",
    "\n",
    "```python\n",
    "def sum(a, axis=None, dtype=None, keepdims=False, split_every=None, out=None):\n",
    "    if dtype is None:\n",
    "        dtype = getattr(np.zeros(1, dtype=a.dtype).sum(), \"dtype\", object)\n",
    "    result = reduction(\n",
    "        a,\n",
    "        chunk.sum,  # this is just `np.sum`\n",
    "        chunk.sum,  # this is just `np.sum`\n",
    "        axis=axis,\n",
    "        keepdims=keepdims,\n",
    "        dtype=dtype,\n",
    "        split_every=split_every,\n",
    "        out=out,\n",
    "    )\n",
    "    return result\n",
    "```\n",
    "[source](https://github.com/dask/dask/blob/ac1bd05cfd40207d68f6eb8603178d7ac0ded922/dask/array/reductions.py#L344-L357)\n",
    "\n",
    "Here is `da.sum` reimplemented as a custom reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab1d5a-c839-4b7c-8fed-9bf88bfa12d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.reduction(x, np.sum, np.sum, dtype=x.dtype).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-display",
   "metadata": {},
   "source": [
    "By visualizing `b` we can see how the tree reduction works. First ``sum`` is applied to each block, then every 4 chunks are combined using ``sum-partial``. This keeps going until there are less than 4 results left, then ``sum-aggregate`` is used to finish up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-heritage",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "See how the graph changes when you set the chunks - maybe to `(100, 250)` or `(250, 250)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a74a2-4e02-4807-b4d1-2221818ef95d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "x = da.random.random(size=(1_000, 1_000), chunks=(100, 250))\n",
    "x.sum().visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0f3153-ac83-46c3-abaa-3e487a81c0b4",
   "metadata": {},
   "source": [
    "### Understanding ``split_every``\n",
    "\n",
    "``split_every`` controls the number of chunk outputs that are used as input to each ``partial`` call. \n",
    "\n",
    "Here is an example of doing partial aggregation on every 5 blocks along the 0 axis and every 2 blocks along the 1 axis (so 10 blocks go into each `partial-sum`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57621ca-58c4-4506-964f-9cf53746e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sum(split_every={0: 5, 1: 2}).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b2b79-af4d-4d79-bd2e-daa90e80bcd5",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Try setting different values for `split_every` and visualizing the task graph to see the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b1dcaa-83fa-474f-9351-dc443500595b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "x.sum(split_every={0: 10, 1: 2}).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5bd9c4-5aaf-4e09-b655-210bf0cc3169",
   "metadata": {},
   "source": [
    "> **Side note**\n",
    ">\n",
    "> You can use reductions to calculate aggregations per-block reduction even if you don't want to combine and aggregate the results of those blocks:\n",
    ">\n",
    "> ```python\n",
    "> da.reduction(x, np.sum, lambda x, **kwargs: x, dtype=int).compute()\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-oasis",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Groupby Aggregation\n",
    "\n",
    "A different but powerful form of reduction is groupby aggregations.\n",
    "\n",
    "There are many standard reductions supported by default on dataframe groupbys. These include methods like `mean, max, min, sum, nunique`. These are easily scaled and parallelized.\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "   - [DataFrame Groupby](https://docs.dask.org/en/latest/dataframe-groupby.html#aggregate)\n",
    "   - [Examples](https://examples.dask.org/dataframes/02-groupby.html)\n",
    "\n",
    "\n",
    "![groupby aggregation](images/custom_operations_groupby_aggregation.png)\n",
    "\n",
    "In order to do that you need to write three functions. These are analogous to the functions that we use for general-purpose `reduction`\n",
    "\n",
    "- `chunk`: operates on the series groupby on each individual partition (`ddf.partitions[0].groupby(\"name\")[\"x\"]`)\n",
    "- `agg`: operates on the concatenated output from calling chunk on every partition\n",
    "- `finalize`: operates on the output from calling aggregate - returns one column. This one is actually optional.\n",
    "\n",
    "Here's an example of a custom aggregation for calculating the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "ddf = dask.datasets.timeseries()\n",
    "\n",
    "custom_mean = dd.Aggregation(\n",
    "    'custom_mean',\n",
    "    lambda s: (s.count(), s.sum()),\n",
    "    lambda count, sum: (count.sum(), sum.sum()),\n",
    "    lambda count, sum: sum / count,\n",
    ")\n",
    "custom_result = ddf.groupby('name').agg(custom_mean)\n",
    "custom_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-defense",
   "metadata": {},
   "source": [
    "Here is how it works:\n",
    "\n",
    "- for every partition (one per day) group by ``name``\n",
    "- on each of those pandas series groupby objects calculate the `count` and the `sum`\n",
    "- concatenate every 8 (this is configurable) outputs together\n",
    "- sum each of these\n",
    "- finally: divide the `sum` by the `count`\n",
    "\n",
    "This is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_result = ddf.groupby('name').mean()\n",
    "simple_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-lawrence",
   "metadata": {},
   "source": [
    "**NOTE**: If you look at the task graph you'll see that the structure of the computation is actually pretty different. That's because `.mean` computes the `sum` and the `count` independently and only combines the values at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_result.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_result.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-shirt",
   "metadata": {},
   "source": [
    "#### Why not use apply?\n",
    "\n",
    "If you are trying to run a custom function on the groups in a groupby it can be tempting to use `.apply` but this is often a poor choice because it requires that the data be shuffled. Instead you should try writing a custom aggregation. Custom aggregation is preferable beacause, each partition is grouped (without being repartitioned) and then the results from each group on each partition are aggregated.\n",
    "\n",
    "DON'T DO:\n",
    "```python\n",
    "ddf.groupby(\"name\").apply(lambda x: x.mean())\n",
    "```\n",
    "\n",
    "This will shuffle the data so that all the data for a particular name is in the same partition. If you call `.compute()` on it you'll notice that it's much slower (about 50x on my computer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea37d15-f26e-4361-be1a-33e71d3d682e",
   "metadata": {},
   "source": [
    "## dask.delayed\n",
    "\n",
    "You can wrap arbitrary functions in dask.delayed to parallelize them, but when you are operating on a Dask collection or several Dask collections, dask.delayed won't understand the organization of your blocks. You need to first can convert your collections into individual blocks set up the computation, and afterwards arrange those blocks as you like. Note that this is often the slowest approach.\n",
    "\n",
    "Let's look back at some of our original computation and implement `da.random.random` and `transpose` using `dask.delayed`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8d50b-fdf4-444a-bcfb-6fa91c8a0932",
   "metadata": {},
   "source": [
    "### Construct an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace26e6-ed88-4c86-b149-5974782e0047",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.random.random(size=(1_000, 1_000), chunks=(250, 500))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dfbea0-4b24-46b3-992e-74a673b995d9",
   "metadata": {},
   "source": [
    "We looked at how you can implement this using `map_blocks` at the top of this notebook. Now let's implement random using `dask.delayed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271311f-6359-4a49-8420-3af3c841ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def random_sample():\n",
    "    return np.random.random(size=(250, 500))\n",
    "\n",
    "da.concatenate(\n",
    "    (da.concatenate(\n",
    "        (da.from_delayed(random_sample(), shape=(250, 500), dtype=float) \n",
    "        for i in range(4)), axis=0\n",
    "    ) \n",
    "    for i in range(2)), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb3c4e-6f94-497d-95f4-010c0c8a2059",
   "metadata": {},
   "source": [
    "### Operate on an existing array\n",
    "There we saw how to create an array from scratch, now let's look at how to operate on an existing array. Let's use delayed to compute the transpose of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbeb1f7-2e56-43c8-90ea-aba70de93ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091deb99-1c80-412d-94bd-379ef2bd023b",
   "metadata": {},
   "source": [
    "First we'll set up the delayed function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66458052-eff0-48cb-8b9a-92b67aae4a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def transpose(block):\n",
    "    return np.transpose(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe601479-aced-49eb-8a61-9545ccc099a7",
   "metadata": {},
   "source": [
    "**Naive approach** - You can just pass the array right in to this function, but when you do the array is first merged into one large array and then the delayed function is applied to the whole array all at once. This can be very nice if your array isn't too large, or if your delayed function can't be coerced to work in per-block. \n",
    "\n",
    "Let's compare the task graphs for calling `x.T` and `transpose(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d5044-bbb5-47d0-90aa-186fa91756dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.T.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c69d321-b9fe-4c52-87bb-e8dded94feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transpose(x).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0257e40-089c-4790-a6b0-66baaec4ebd6",
   "metadata": {},
   "source": [
    "The better approach is to first convert the array to a nested list of delayed objects - there will be one delayed object to represent each block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8b4437-4352-4155-b8f5-27f645785551",
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_array = x.to_delayed()\n",
    "delayed_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ac1718-67f7-4361-8447-325c9d8eab02",
   "metadata": {},
   "source": [
    "Then you can iterate over that nested list of delayed objects, convert the output to arrays and concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a819a-c359-4eed-acb7-2e7990bc911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = da.concatenate(\n",
    "    (da.concatenate(\n",
    "        (da.from_delayed(transpose(block), shape=(500, 250), dtype=float) \n",
    "        for block in row), \n",
    "        axis=0)\n",
    "    for row in delayed_array),\n",
    "    axis=1\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00423d90-14ef-4c25-a057-948ec1ba767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-snapshot",
   "metadata": {},
   "source": [
    "## When to use which method\n",
    "\n",
    "In this notebook we've covered several different mechanisms for applying arbitrary functions to the blocks in arrays or dataframes. Here's a brief summary of when you should use these various methods\n",
    "\n",
    "- `map_block`, `map_partition` - block organization of the input matches the block organization of the output and the function is fully parallelizable. \n",
    "- `map_overlap` - block organizations of input and output match, but the function is not fully parallelizable (requires input from neighboring chunks).\n",
    "- `blockwise` - same function can be applied to the blocks as to the partial and aggregated versions. Also output blocks can be in different orientations.\n",
    "- `reduction` - dimensionality of output does not necessarily match that of input and function is fully parallelizable.\n",
    "- `groupby().agg` - data needs to be aggregated per group (the index of the output will be the group keys).\n",
    "- `dask.delayed` - data doesn't have a complex block organization or the data is small and the computation is pretty fast."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
