{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "negative-minority",
   "metadata": {},
   "source": [
    "# Advanced Collections: _Custom Operations_\n",
    "\n",
    "In the overview notebook we discussed some of the many algorithms that are pre-defined for different types of Dask collections\n",
    "(such as Arrays, DataFrames and Bags). These include operations like `mean`, `max`, `value_counts` and many other standard operations.\n",
    "\n",
    "Sometimes for more niche use-cases, these predefined\n",
    "algorithms are not sufficient. \n",
    "\n",
    "You can wrap arbitrary functions in ``dask.delayed`` to parallelize them,\n",
    "but when you are operating on a Dask collection or several Dask collections,\n",
    "``dask.delayed`` won't understand the organization of your blocks. \n",
    "\n",
    "In these cases there are several different ways in which you can set up\n",
    "custom functions.\n",
    "\n",
    "-   All collections have a ``map_partitions`` or ``map_blocks`` function, that\n",
    "    applies a user provided function across every Pandas dataframe or NumPy array\n",
    "    in the collection.  Because Dask collections are made up of normal Python\n",
    "    objects, it's often straightforward to map custom functions across partitions of a\n",
    "    dataset without much modification.\n",
    "\n",
    "-   More complex ``map_*`` functions.  Sometimes your custom behavior isn't\n",
    "    embarrassingly parallel, but requires more advanced communication.  For\n",
    "    example maybe you need to communicate a little bit of information from one\n",
    "    partition to the next, or maybe you want to build a custom aggregation.\n",
    "\n",
    "    Dask collections include methods for these as well.\n",
    "\n",
    "-   For even more complex workloads you can convert your collections into\n",
    "    individual blocks, and arrange those blocks as you like using Dask Delayed.\n",
    "    There is usually a ``to_delayed`` method on every collection. Note that this\n",
    "    is often the slowest approach.\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "  - [Array Tutorial](https://tutorial.dask.org/03_array.html)\n",
    "  - [Best Practices](https://docs.dask.org/en/latest/best-practices.html#learn-techniques-for-customization)\n",
    "\n",
    "## Block Computations\n",
    "Block computations operate on a per-block basis. So each block gets the function applied to it, and the output has the same chunk pattern as the input.\n",
    "\n",
    "![map_blocks](images/custom_operations_map_blocks.png)\n",
    "\n",
    "\n",
    "They are simple to think about, but can be tricky to get right.\n",
    "We'll explore some best practices for setting them up.\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "   - [`dask.array.map_blocks`](https://docs.dask.org/en/latest/array-api.html?highlight=map_blocks#dask.array.Array.map_blocks)\n",
    "   - [`dask.dataframe.map_partitions`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-albert",
   "metadata": {},
   "source": [
    "### Array\n",
    "\n",
    "Here is a straightforward case of `map_blocks` on a Dask array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-invalid",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "a = da.arange(0, 6, chunks=3)\n",
    "\n",
    "result = a.map_blocks(lambda x: x ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-computer",
   "metadata": {},
   "source": [
    "When we compute the result, we see that every item in ``a`` is squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-justice",
   "metadata": {},
   "source": [
    "This is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-architect",
   "metadata": {},
   "outputs": [],
   "source": [
    "same = a ** 2\n",
    "same.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-evidence",
   "metadata": {},
   "source": [
    "We can use ``.visualize`` to convince ourselves that the task graph for both these\n",
    "operations has the same structure. Two entirely independent \"towers\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "same.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-undergraduate",
   "metadata": {},
   "source": [
    "This function was easy to understand and apply to each block. When the operation is more convoluted it can help to start by writing a function that works as expected on _one block_ before passing it to ``map_blocks``.\n",
    "\n",
    "Let's write a function that takes the first element of the block and subtracts it from all the following items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(block):\n",
    "    return block - block[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-quest",
   "metadata": {},
   "source": [
    "Test the function out on any block of ``a``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "func(a.blocks[1]).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-hacker",
   "metadata": {},
   "source": [
    "> NOTE: Since `a.blocks[1]` is actually a dask.array, you have to call compute to get the real result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-exclusive",
   "metadata": {},
   "source": [
    "Now pass that function to ``map_blocks``. Notice that each block has the same output: `array([0, 1, 2])` and ``map_blocks`` stacks the outputs so that the chunks are in the same pattern as the input chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.map_blocks(func, a).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-sigma",
   "metadata": {},
   "source": [
    "Since our funciton depends on the value of the first item in the any particular chunk, if you rechunk the array and call `map_blocks` on that new array, you'll get a different result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-local",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.map_blocks(func, a.rechunk(2)).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-devon",
   "metadata": {},
   "source": [
    "#### Multiple arrays\n",
    "\n",
    "Map_blocks can be used to combine several Dask arrays. When multiple arrays are passed, ``map_blocks``\n",
    "aligns blocks by block positions without regard to shape.\n",
    "\n",
    "In the following example we have two arrays with the same number of blocks\n",
    "but with different shape and chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "x = da.arange(1000, chunks=(100,))\n",
    "y = da.arange(100, chunks=(10,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-maldives",
   "metadata": {},
   "source": [
    "Let's take a look at these arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-excess",
   "metadata": {},
   "source": [
    "We can pass these arrays into ``map_blocks`` using a funciton that takes two inputs, calculates the max of each, than then returns a numpy array of the ou the puts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(a, b):\n",
    "    return np.array([a.max(), b.max()])\n",
    "\n",
    "result = da.map_blocks(func, x, y, chunks=(2,))\n",
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-congo",
   "metadata": {},
   "source": [
    "#### Understanding `chunks` argument\n",
    "\n",
    "In the example above we explicitly declare what the size of the output chunks will be ``chunks=(2,)`` this\n",
    "is short-hand for ``chunks=((2, 2, 2, 2, 2, 2, 2, 2, 2, 2,),)`` meaning 10 blocks each with shape ``(2,)``.\n",
    "You can see the shape and chunk information in the array representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-error",
   "metadata": {},
   "source": [
    "> Specifying the output chunks is very useful when doing more involved operations with ``map_blocks``. If you don't specify ``chunks``, the output will not have the right expectation of the final shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-output",
   "metadata": {},
   "source": [
    "#### Special arguments\n",
    "\n",
    "There are special arguments (``block_info`` and ``block_id``) that you can use within ``map_blocks`` functions. ``block_id`` gives the index of the block within the chunks, so for a 1D array it will be something like `(i,)`. ``block_info`` is a dictionary where there is an integer key for each input dask array and a `None` key for the output array.\n",
    "\n",
    "Let's use the example above and print ``block_info`` for the first block so that we can get a sense of what information is contained in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def func(a, b, block_id=None, block_info=None):\n",
    "    if block_id == (0,):\n",
    "        pprint(block_info)\n",
    "    return np.array([a.max(), b.max()])\n",
    "\n",
    "da.map_blocks(func, x, y, chunks=(2,)).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-kentucky",
   "metadata": {},
   "source": [
    "One of the use cases for the ``block_info`` and ``block_id`` arguments is to create an array from scratch. In the following example we first create an empty array with the desired number of chunks: 10x10. Then we\n",
    "fill each chunk with data based on the starting positions of that block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "x = da.empty(100, shape=(10, 10), chunks=(1, 1))\n",
    "\n",
    "def generate_data(a, block_id=None):\n",
    "    ii, jj = block_id\n",
    "    return np.arange(ii*200, (ii+1)*200).reshape((10, 20))\n",
    "\n",
    "da.map_blocks(generate_data, x, chunks=(10, 20), dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-lover",
   "metadata": {},
   "source": [
    "This example might feel contrived, but it can be useful when creating custom IO operations\n",
    "especially in a distributed context.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Say you have a set of images that each represent a particular portion of a scene. How can you use the\n",
    "technique we just learned to patch them together? \n",
    "\n",
    "Let's look at what is in the puzzle directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls puzzle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-expansion",
   "metadata": {},
   "source": [
    "The following cell displays the completed puzzle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = imread(\"puzzle/bicycle.png\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-stake",
   "metadata": {},
   "source": [
    "Now use ``map_blocks`` to read in the puzzle pieces from \"bicycle_i_j.png\". Note that each image piece has 3 dimensions: x, y, and RGBA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first set up an empty array with the right shape and one chunk per puzzle piece\n",
    "a = da.empty(shape=(2, 2, 4), chunks=((1, 1), (1, 1), (4,)))\n",
    "\n",
    "# define a function that reads one file\n",
    "def reader(block, block_id=None):\n",
    "    ... = block_id  # unpack block_id to get chunk index values\n",
    "    filename = ...  # use chunk index values to get the correct file\n",
    "    return imread(filename)\n",
    "\n",
    "# map that function to every block and set the expected dtype of the output\n",
    "result = da.map_blocks(reader, a, dtype=int)\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-daughter",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "# first set up an empty array with the right shape and one chunk per puzzle piece\n",
    "a = da.empty(shape=(2, 2, 4), chunks=((1, 1), (1, 1), (4,)))\n",
    "\n",
    "# define a function that reads one file\n",
    "def reader(block, block_id=None):\n",
    "    ii, jj, _ = block_id                        # unpack block_id to get chunk index values\n",
    "    filename = f\"puzzle/bicycle_{ii}_{jj}.png\"  # use chunk index values to get the correct file\n",
    "    return imread(filename)\n",
    "\n",
    "# map that function to every block and set the expected dtype of the output\n",
    "result = da.map_blocks(reader, a, dtype=int)\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-interim",
   "metadata": {},
   "source": [
    "### DataFrame\n",
    "\n",
    "In Dask dataframe there is a similar method to ``map_blocks`` but it is called ``map_partitions``.\n",
    "\n",
    "Here is an example of using it to check if the sum of two columns is greater than some\n",
    "arbitrary threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "df = pd.DataFrame({'x': [1, 2, 3, 4, 5],\n",
    "                   'y': [6.4, 7.7, 8.3, 9.1, 10.2]})\n",
    "ddf = dd.from_pandas(df, npartitions=2)\n",
    "\n",
    "result = ddf.map_partitions(lambda df, threshold: (df.x + df.y) > threshold, threshold=9)\n",
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-preparation",
   "metadata": {},
   "source": [
    "#### Internal uses\n",
    "In practice ``map_partitions`` is used to implement many of the helper dataframe methods\n",
    "that let Dask dataframe mimic Pandas. Here is the implementation of `ddf.index` for instance:\n",
    "\n",
    "```python\n",
    "@property\n",
    "def index(self):\n",
    "    \"\"\"Return dask Index instance\"\"\"\n",
    "    return self.map_partitions(\n",
    "        getattr,\n",
    "        \"index\",\n",
    "        token=self._name + \"-index\",\n",
    "        meta=self._meta.index,\n",
    "        enforce_metadata=False,\n",
    "    )\n",
    "```\n",
    "\n",
    "[source](https://github.com/dask/dask/blob/09862ed99a02bf3a617ac53b116f9ecf81eea338/dask/dataframe/core.py#L458-L467)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-norwegian",
   "metadata": {},
   "source": [
    "### Understanding `meta`\n",
    "\n",
    "Dask dataframes and dask arrays have a special attribute called `_meta` that allows them to know metadata about the type of dataframe/array that they represent. This metadata includes:\n",
    " - dtype (int, float)\n",
    " - column names and order\n",
    " - name\n",
    " - type (pandas dataframe, cudf dataframe)\n",
    " \n",
    "**Related documentation**\n",
    "\n",
    "- [Dataframe metadata](https://docs.dask.org/en/latest/dataframe-design.html#metadata)\n",
    "\n",
    "This information is stored in an empty object of the proper type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ddf._meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-architect",
   "metadata": {},
   "source": [
    "That's how dask knows what to render when you display a dask object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-prairie",
   "metadata": {},
   "source": [
    "When you add an item to the task graph, Dask tries to run the function on the meta before you call compute. \n",
    "\n",
    "This approach has several benefits:\n",
    "\n",
    "- it gives Dask a sense of what the output will look like. \n",
    "- if there are fundamental issues, Dask will fail fast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-excitement",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-memory",
   "metadata": {},
   "source": [
    "Sometimes running the function on a miniature version of the data doesn't produce a result that is similar enough to your expected output. \n",
    "\n",
    "In those cases you can provide a `meta` manually - this will save you the step of running the function on graph creation, so it will be slightly faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "result = ddf.map_partitions(lambda df, threshold: (df.x + df.y) > threshold, threshold=9, meta=bool)\n",
    "result.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "result = ddf.map_partitions(lambda df, threshold: (df.x + df.y) > threshold, threshold=9)\n",
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-sharing",
   "metadata": {},
   "source": [
    "### When not to use ``map_blocks`` or ``map_partitions``\n",
    "\n",
    "Both ``map_blocks`` and ``map_partitions`` operate on each block in isolation. This\n",
    "makes them ill-suited for operations that depend on outcomes in other chunks.\n",
    "It also means that there will always be at least one result per block.\n",
    "\n",
    "When you need the edges of one block in the next block you can use overlapping computations.\n",
    "\n",
    "## Overlapping Computations\n",
    "Sometimes you want to operate on a per-block basis, but you need some information from neighboring blocks. Example operations include the following:\n",
    "\n",
    "- Convolve a filter across an image\n",
    "- Rolling sum/mean/max, …\n",
    "- Search for image motifs like a Gaussian blob that might span the border of a block\n",
    "- Evaluate a partial derivative\n",
    "\n",
    "Dask Array supports these operations by creating a new array where each block is slightly expanded by the borders of its neighbors. \n",
    "\n",
    "![](https://docs.dask.org/en/latest/_images/overlapping-neighbors.png)\n",
    "\n",
    "This costs an excess copy and the communication of many small chunks, but allows localized functions to evaluate in an embarrassingly parallel manner.\n",
    "\n",
    "**Related Documentation**\n",
    "   - [Array Overlap](https://docs.dask.org/en/latest/array-overlap.html)\n",
    "\n",
    "The main API for these computations is the ``map_overlap`` method. ``map_overlap`` is very similar to ``map_blocks`` but has the additional arguments: ``depth``, ``boundary``, and ``trim``.\n",
    "\n",
    "Here is an example of calculating the derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "x = np.array([1, 1, 2, 3, 3, 3, 2, 1, 1])\n",
    "x = da.from_array(x, chunks=5)\n",
    "\n",
    "def derivative(x):\n",
    "    return x - np.roll(x, 1)\n",
    "\n",
    "y = x.map_overlap(derivative, depth=1, boundary=0)\n",
    "y.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-lying",
   "metadata": {},
   "source": [
    "In this case each block shares 1 value from its neighboring block: ``depth``. And since we set ``boundary=0``on the outer edges of the array, the first and last block are padded with the integer 0. Since we haven't specified ``trim`` it is true by default meaning that the overlap is removed before returning the results.\n",
    "\n",
    "If you inspect the task graph you'll see two mostly independent towers of tasks, with just some value sharing at the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.visualize(optimize_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-crowd",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Lets apply a gaussian filter to an image following the example from the [scipy docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.gaussian_filter.html).\n",
    "\n",
    "First create a dask array from the numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "import dask.array as da\n",
    "\n",
    "a = da.from_array(misc.ascent(), chunks=(128, 128))\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-editing",
   "metadata": {},
   "source": [
    "Now use ``map_overlap`` to apply ``gausian_filter`` to each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "b = a.map_overlap(gaussian_filter, sigma=5, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-worcester",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "b = a.map_overlap(gaussian_filter, sigma=5, depth=10, boundary=\"periodic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-anaheim",
   "metadata": {},
   "source": [
    "Check what you've come up with by plotting the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(a)\n",
    "ax2.imshow(b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-mistake",
   "metadata": {},
   "source": [
    "> Notice that if you set the depth to a smaller value, you can see the edges of the blocks in the output image.\n",
    "\n",
    "## Blockwise Computations\n",
    "\n",
    "Blockwise computations provide the infrastructure for implementing ``map_blocks`` and many\n",
    "of the elementwise methods that make up the Array API.\n",
    "\n",
    "They present a really powerful way of implementing matrix operations.\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "   - [API Documentation](https://docs.dask.org/en/latest/array-api.html#dask.array.blockwise)\n",
    "   \n",
    "\n",
    "Let's look at an example of summing two arrays blockwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "x = da.from_array([[1, 2],\n",
    "                   [3, 4]], chunks=(1, 2))\n",
    "y = da.from_array([[10, 20],\n",
    "                   [0, 0]])\n",
    "z = da.blockwise(operator.add, 'ij', x, 'ij', y, 'ij')\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-fiction",
   "metadata": {},
   "source": [
    "Try switching the block pattern of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = da.blockwise(operator.add, 'ji', x, 'ij', y, 'ij')\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-repeat",
   "metadata": {},
   "source": [
    "In each of these case the outcome for each block is the same. But in the first case\n",
    "the blocks are stacked vertically (`shape=(2, 2)`) and in the second they are placed \n",
    "side-by-side (`shape=(1, 4)`) \n",
    "\n",
    "\n",
    "![Clockwise custom operations](images/custom_operations_blockwise.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-hurricane",
   "metadata": {},
   "source": [
    "### Internal uses\n",
    "\n",
    "This is the internal definition of transpose on dask.Array. In it you can see that there is a\n",
    "regular ``np.transpose`` applied within each block and then the blocks are themselves transposed.\n",
    "\n",
    "```python\n",
    "def transpose(a, axes=None):\n",
    "    if axes:\n",
    "        if len(axes) != a.ndim:\n",
    "            raise ValueError(\"axes don't match array\")\n",
    "    else:\n",
    "        axes = tuple(range(a.ndim))[::-1]\n",
    "    axes = tuple(d + a.ndim if d < 0 else d for d in axes)\n",
    "    return blockwise(\n",
    "        np.transpose, axes, a, tuple(range(a.ndim)), dtype=a.dtype, axes=axes\n",
    "    )\n",
    "```\n",
    "\n",
    "[source](https://github.com/dask/dask/blob/4569b150db36af0aa9d9a8d318b4239a78e2eaec/dask/array/routines.py#L161:L170)\n",
    "\n",
    "## Reduction\n",
    "All of the methods that we have covered so far assume that the output array will be similar to the input array. But where dask really shines is with aggregations. Each dask collection has a `reduction` method that is the generalized method that supports operations that reduce the dimensionality of the inputs.\n",
    "\n",
    "![Custom operations: reduction](images/custom_operations_reduction.png)\n",
    "\n",
    "**Related Documentation**\n",
    "   - [`dask.array.reduction`](http://dask.pydata.org/en/latest/array-api.html#dask.dataframe.Array.reduction)\n",
    "   - [`dask.dataframe.reduction`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-representation",
   "metadata": {},
   "source": [
    "### Internal uses\n",
    "\n",
    "This is the internal definition of sum on dask.Array. In it you can see that there is a\n",
    "regular ``np.sum`` applied across each block and then tree-reduced with ``np.sum`` again.\n",
    "\n",
    "```python\n",
    "def sum(a, axis=None, dtype=None, keepdims=False, split_every=None, out=None):\n",
    "    if dtype is None:\n",
    "        dtype = getattr(np.zeros(1, dtype=a.dtype).sum(), \"dtype\", object)\n",
    "    result = reduction(\n",
    "        a,\n",
    "        chunk.sum,  # this is just `np.sum`\n",
    "        chunk.sum,  # this is just `np.sum`\n",
    "        axis=axis,\n",
    "        keepdims=keepdims,\n",
    "        dtype=dtype,\n",
    "        split_every=split_every,\n",
    "        out=out,\n",
    "    )\n",
    "    return result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-virginia",
   "metadata": {},
   "source": [
    "In this case the  ``chunk``, ``combine`` and ``aggregate`` functions are all the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "a = da.from_array(np.arange(1000000).reshape(1000, 1000), chunks=(500, 200))\n",
    "b = a.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-display",
   "metadata": {},
   "source": [
    "By visualizing `b` we can see how the tree reduction works. First the ``chunk`` function is applied to each block, then every 4 chunks are combined using the ``combine`` function. This keeps going until there are only 2 chunks left, then the ``aggregate`` function is used to finish up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-heritage",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "See how the graph changes when you set the chunks to `(500, 500)` or `(500, 300)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-mexican",
   "metadata": {},
   "source": [
    "### Controlling reduction with kwargs\n",
    "\n",
    "There are a few handy keyword arguments that you can use to control the shape of the task graph: ``keepdims``, ``out``, ``split_every``...\n",
    "\n",
    "The most useful of these is ``split_every`` which controls the number of chunk outputs that are used as input to each ``combine`` call. Try setting `split_every` on `a.sum()` like ``a.sum(split_every={0: 2, 1: 5})`` and visualizing the task graph to see the impact.\n",
    "\n",
    "> **Side note**\n",
    ">\n",
    "> You can use reductions to calculate aggregations per-block reduction even if you don't want to combine and aggregate the results of those blocks:\n",
    ">\n",
    "> ```python\n",
    "> da.reduction(a, np.sum, lambda x, **kwargs: x, dtype=int).compute()\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-oasis",
   "metadata": {},
   "source": [
    "## Groupby Aggregation\n",
    "\n",
    "There are many standard reductions supported by default on dataframe groupbys. These include methods like `mean, max, min, sum, nunique`. These are easily scaled and parallelized.\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "   - [DataFrame Groupby](https://docs.dask.org/en/latest/dataframe-groupby.html#aggregate)\n",
    "   - [Examples](https://examples.dask.org/dataframes/02-groupby.html)\n",
    "\n",
    "If you are trying to run a custom function on the groups in a groupby it can be tempting to use `.apply` but this is often a poor choice because it requires that the data be shuffled. Instead you should try writing a custom aggregation.\n",
    "\n",
    "When you use a custom aggregation, each partition is grouped (without being repartitioned) and then the results from each group on each partition are aggregated.\n",
    "\n",
    "![groupby aggregation](images/custom_operations_groupby_aggregation.png)\n",
    "\n",
    "In order to do that you need to write three functions:\n",
    "\n",
    "- `chunk`: operates on the series groupby on each individual partition (`ddf.partitions[0].groupby(\"name\")[\"x\"]`)\n",
    "- `agg`: operates on the concatenated output from calling chunk on every partition\n",
    "- `finalize`: operates on the output from calling aggregate - returns one column. This one is actually optional.\n",
    "\n",
    "Here's an example of a custom aggregation for calculating the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "ddf = dask.datasets.timeseries()\n",
    "\n",
    "custom_mean = dd.Aggregation(\n",
    "    'custom_mean',\n",
    "    lambda s: (s.count(), s.sum()),\n",
    "    lambda count, sum: (count.sum(), sum.sum()),\n",
    "    lambda count, sum: sum / count,\n",
    ")\n",
    "custom_result = ddf.groupby('name').agg(custom_mean)\n",
    "custom_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-defense",
   "metadata": {},
   "source": [
    "Here is how it works:\n",
    "\n",
    "- for every partition (one per day) group by ``name``\n",
    "- on each of those pandas series groupby objects calculate the `count` and the `sum`\n",
    "- concatenate every 8 (this is configurable) outputs together\n",
    "- sum each of these\n",
    "- finally: divide the `sum` by the `count`\n",
    "\n",
    "This is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_result = ddf.groupby('name').mean()\n",
    "simple_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-lawrence",
   "metadata": {},
   "source": [
    "**NOTE**: If you look at the task graph you'll see that the structure of the computation is actually pretty different. That's because `.mean` computes the `sum` and the `count` independently and only combines the values at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_result.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_result.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-shirt",
   "metadata": {},
   "source": [
    "Similarly you could use apply (**DON'T DO THIS**)\n",
    "\n",
    "```python\n",
    "ddf.groupby(\"name\").apply(lambda x: x.mean())\n",
    "```\n",
    "\n",
    "This will shuffle the data so that all the data for a particular name is in the same partition. If you call `.compute()` on it you'll notice that it's much slower (about 50x on my computer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-florida",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Write a custom aggregation to calculate the `count` of rows for each `name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_count = dd.Aggregation(\n",
    "    'custom_count',\n",
    "    chunk=...,  # write a function that operates on one chunk and gives the count\n",
    "    agg=...,    # write a function that operates on the concattenated results from each chunk\n",
    ")\n",
    "ddf.groupby('name').agg(custom_count).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-midnight",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "custom_count = dd.Aggregation(\n",
    "    'custom_ount',\n",
    "    chunk=lambda s: s.count(),\n",
    "    agg=lambda counts: counts.sum(),\n",
    ")\n",
    "ddf.groupby('name').agg(custom_count).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-snapshot",
   "metadata": {},
   "source": [
    "## When to use which method\n",
    "\n",
    "In this notebook we've covered several different mechanisms for applying arbitrary functions to the blocks in arrays or dataframes. Here's a brief summary of when you should use these various methods\n",
    "\n",
    "- `map_block`, `map_partition` - chunk pattern of the input matches the chunk pattern of the output and the function is fully parrallelizable. \n",
    "- `map_overlap` - chunk paterns match, but the function is not fully parallelizable (requires input from neighboring chunks).\n",
    "- `blockwise` - input and output chunks are similar but may be in different orientations.\n",
    "- `reduction` - dimensionality of output does not necessarily match that of input and function is fully parallelizable.\n",
    "- `groupby().agg` - when data needs to be aggregated per group (the index of the output will be the group keys)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
