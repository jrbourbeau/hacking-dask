{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "characteristic-conclusion",
   "metadata": {},
   "source": [
    "# Advanced Collections: _Custom Collections_\n",
    "\n",
    "For many problems, the built-in Dask collections (``dask.array``,\n",
    "``dask.dataframe``, ``dask.bag``, and ``dask.delayed``) are sufficient. For\n",
    "cases where they aren't, it's possible to create your own Dask collections. Here\n",
    "we describe the required methods to fulfill the Dask collection interface.\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "  - [Custom Collections](https://docs.dask.org/en/latest/custom-collections.html)\n",
    "\n",
    "## The Dask Collection Interface\n",
    "\n",
    "To create your own Dask collection, you need to fulfill the following\n",
    "interface. Note that there is no required base class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectionInterface():\n",
    "    def __dask_graph__(self):\n",
    "        \"\"\"The Dask graph.\n",
    "\n",
    "        **Returns**\n",
    "        dsk : Mapping, None\n",
    "            The Dask graph.  If ``None``, this instance will not be interpreted as a\n",
    "            Dask collection, and none of the remaining interface methods will be\n",
    "            called.\n",
    "\n",
    "        If the collection also specifies :meth:`__dask_layers__`, then ``dsk`` must be a\n",
    "        :class:`~dask.highlevelgraph.HighLevelGraph` or ``None``.\n",
    "        \"\"\"\n",
    "\n",
    "    def __dask_keys__(self):\n",
    "        \"\"\"The output keys for the Dask graph.\n",
    "\n",
    "        Note that there are additional constraints on keys for a Dask collection\n",
    "        than those described in the :doc:`task graph specification documentation <spec>`.\n",
    "        These additional constraints are described below.\n",
    "\n",
    "        **Returns**\n",
    "\n",
    "        keys : list\n",
    "            A possibly nested list of keys that represent the outputs of the graph.\n",
    "            After computation, the results will be returned in the same layout,\n",
    "            with the keys replaced with their corresponding outputs.\n",
    "\n",
    "        All keys must either be non-empty strings or tuples where the first element is a\n",
    "        non-empty string, followed by zero or more arbitrary hashables.\n",
    "        The non-empty string is commonly known as the *collection name*. All collections\n",
    "        embedded in the dask package have exactly one name, but this is not a requirement.\n",
    "\n",
    "        These are all valid outputs:\n",
    "\n",
    "        - ``[]``\n",
    "        - ``[\"x\", \"y\"]``\n",
    "        - ``[[(\"y\", \"a\", 0), (\"y\", \"a\", 1)], [(\"y\", \"b\", 0), (\"y\", \"b\", 1)]``\n",
    "        \"\"\"\n",
    "\n",
    "    def __dask_layers__(self):\n",
    "        \"\"\" This method should only be implemented if the collection uses\n",
    "        ``~dask.highlevelgraph.HighLevelGraph`` to implement its dask graph.\n",
    "\n",
    "        **Returns**\n",
    "\n",
    "        names : tuple\n",
    "            Tuple of names of the HighLevelGraph layers which contain all keys returned by\n",
    "            ``__dask_keys__``.\n",
    "        \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def __dask_optimize__(dsk, keys, **kwargs):\n",
    "        \"\"\"\n",
    "        Given a graph and keys, return a new optimized graph.\n",
    "\n",
    "        This method can be either a ``staticmethod`` or a ``classmethod``, but not\n",
    "        an ``instancemethod``.\n",
    "\n",
    "        Note that graphs and keys are merged before calling ``__dask_optimize__``;\n",
    "        as such, the graph and keys passed to this method may represent more than\n",
    "        one collection sharing the same optimize method.\n",
    "\n",
    "        If not implemented, defaults to returning the graph unchanged.\n",
    "\n",
    "        **Parameters**\n",
    "\n",
    "        dsk : MutableMapping\n",
    "            The merged graphs from all collections sharing the same\n",
    "            ``__dask_optimize__`` method.\n",
    "        keys : list\n",
    "            A list of the outputs from ``__dask_keys__`` from all collections\n",
    "            sharing the same ``__dask_optimize__`` method.\n",
    "        **kwargs\n",
    "            Extra keyword arguments forwarded from the call to ``compute`` or\n",
    "            ``persist``. Can be used or ignored as needed.\n",
    "\n",
    "        **Returns**\n",
    "\n",
    "        optimized_dsk : MutableMapping\n",
    "            The optimized Dask graph.\n",
    "        \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def __dask_scheduler__(dsk, keys, **kwargs):\n",
    "        \"\"\"The default scheduler ``get`` to use for this object.\n",
    "\n",
    "        Usually attached to the class as a staticmethod, e.g.:\n",
    "\n",
    "        >>> import dask.threaded\n",
    "        >>> class MyCollection:\n",
    "        ...     # Use the threaded scheduler by default\n",
    "        ...     __dask_scheduler__ = staticmethod(dask.threaded.get)\n",
    "        \"\"\"\n",
    "\n",
    "    def __dask_postcompute__(self):\n",
    "        \"\"\"Return the finalizer and (optional) extra arguments to convert the computed\n",
    "        results into their in-memory representation.\n",
    "\n",
    "        Used to implement ``dask.compute``.\n",
    "\n",
    "        **Returns**\n",
    "\n",
    "        finalize : callable\n",
    "            A function with the signature ``finalize(results, *extra_args)``.\n",
    "            Called with the computed results in the same structure as the\n",
    "            corresponding keys from ``__dask_keys__``, as well as any extra\n",
    "            arguments as specified in ``extra_args``. Should perform any necessary\n",
    "            finalization before returning the corresponding in-memory collection\n",
    "            from ``compute``. For example, the ``finalize`` function for\n",
    "            ``dask.array.Array`` concatenates all the individual array chunks into\n",
    "            one large numpy array, which is then the result of ``compute``.\n",
    "        extra_args : tuple\n",
    "            Any extra arguments to pass to ``finalize`` after ``results``. If no\n",
    "            extra arguments should be an empty tuple.\n",
    "        \"\"\"\n",
    "\n",
    "    def __dask_postpersist__(self):\n",
    "        \"\"\"Return the rebuilder and (optional) extra arguments to rebuild an equivalent\n",
    "        Dask collection from a persisted or rebuilt graph.\n",
    "\n",
    "        Used to implement :func:`dask.persist`.\n",
    "\n",
    "        **Returns**\n",
    "\n",
    "        rebuild : callable\n",
    "            A function with the signature\n",
    "            ``rebuild(dsk, *extra_args, rename : Mapping[str, str] = None)``.\n",
    "            ``dsk`` is a Mapping which contains at least the output keys returned by\n",
    "            :meth:`__dask_keys__`. The callable should return an equivalent Dask collection\n",
    "            with the same keys as ``self``, but with the results that are computed through a\n",
    "            different graph. In the case of :func:`dask.persist`, the new graph will have\n",
    "            just the output keys and the values already computed.\n",
    "\n",
    "            If the optional parameter ``rename`` is specified, it indicates that output\n",
    "            keys may be changing too; e.g. if the previous output of :meth:`__dask_keys__`\n",
    "            was ``[(\"a\", 0), (\"a\", 1)]``, after calling\n",
    "            ``rebuild(dsk, *extra_args, rename={\"a\": \"b\"})`` it must become\n",
    "            ``[(\"b\", 0), (\"b\", 1)]``.\n",
    "            The ``rename`` mapping may not contain the collection name(s); in such case the\n",
    "            associated keys do not change. It may contain replacements for unexpected names,\n",
    "            which must be ignored.\n",
    "\n",
    "        extra_args : tuple\n",
    "            Any extra arguments to pass to ``rebuild`` after ``dsk``. If no extra\n",
    "            arguments are necessary, it must be an empty tuple.\n",
    "        \"\"\"\n",
    "\n",
    "    def __dask_tokenize__(self):\n",
    "        \"\"\"Optional, but recommended for implementing deterministic hashing.\n",
    "\n",
    "        Where possible, it is recommended to define the ``__dask_tokenize__`` method.\n",
    "        This method takes no arguments and should return a value fully\n",
    "        representative of the object.\n",
    "\n",
    "        **Returns**\n",
    "\n",
    "        token : string\n",
    "            A unique identifier for this collection. Used as the key in dask graph.\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-tourism",
   "metadata": {},
   "source": [
    "## Internals of the Core Dask Methods\n",
    "\n",
    "Dask has a few *core* functions (and corresponding methods) that implement\n",
    "common operations:\n",
    "\n",
    "- [``compute``](#compute): Convert one or more Dask collections into their in-memory\n",
    "  counterparts\n",
    "- [``persist``](#persist): Convert one or more Dask collections into equivalent Dask\n",
    "  collections with their results already computed and cached in memory\n",
    "- [``optimize``](#optimize): Convert one or more Dask collections into equivalent Dask\n",
    "  collections sharing one large optimized graph\n",
    "- [``visualize``](#visualize): Given one or more Dask collections, draw out the graph that\n",
    "  would be passed to the scheduler during a call to ``compute`` or ``persist``\n",
    "- [``tokenize``]: Generate key for collection using Dask's own deterministic hash function.\n",
    "\n",
    "Here we briefly describe the internals of these functions to illustrate how\n",
    "they relate to the above interface.\n",
    "\n",
    "### Compute\n",
    "\n",
    "The operation of ``compute`` can be broken into three stages:\n",
    "\n",
    "1. **Graph Merging & Optimization**\n",
    "\n",
    "   First, the individual collections are converted to a single large graph and\n",
    "   nested list of keys. How this happens depends on the value of the\n",
    "   ``optimize_graph`` keyword, which each function takes:\n",
    "\n",
    "   - If ``optimize_graph`` is ``True`` (default), then the collections are first\n",
    "     grouped by their ``__dask_optimize__`` methods.  All collections with the\n",
    "     same ``__dask_optimize__`` method have their graphs merged and keys\n",
    "     concatenated, and then a single call to each respective\n",
    "     ``__dask_optimize__`` is made with the merged graphs and keys.  The\n",
    "     resulting graphs are then merged.\n",
    "\n",
    "   - If ``optimize_graph`` is ``False``, then all the graphs are merged and all\n",
    "     the keys concatenated.\n",
    "\n",
    "   After this stage there is a single large graph and nested list of keys which\n",
    "   represents all the collections.\n",
    "\n",
    "2. **Computation**\n",
    "\n",
    "   After the graphs are merged and any optimizations performed, the resulting\n",
    "   large graph and nested list of keys are passed on to the scheduler.  The\n",
    "   scheduler to use is chosen as follows:\n",
    "\n",
    "   - If a ``get`` function is specified directly as a keyword, use that\n",
    "   - Otherwise, if a global scheduler is set, use that\n",
    "   - Otherwise fall back to the default scheduler for the given collections.\n",
    "     Note that if all collections don't share the same ``__dask_scheduler__``\n",
    "     then an error will be raised.\n",
    "\n",
    "   Once the appropriate scheduler ``get`` function is determined, it is called\n",
    "   with the merged graph, keys, and extra keyword arguments.  After this stage,\n",
    "   ``results`` is a nested list of values. The structure of this list mirrors\n",
    "   that of ``keys``, with each key substituted with its corresponding result.\n",
    "\n",
    "3. **Postcompute**\n",
    "\n",
    "   After the results are generated, the output values of ``compute`` need to be\n",
    "   built. This is what the ``__dask_postcompute__`` method is for.\n",
    "   ``__dask_postcompute__`` returns two things:\n",
    "\n",
    "   - A ``finalize`` function, which takes in the results for the corresponding\n",
    "     keys\n",
    "   - A tuple of extra arguments to pass to ``finalize`` after the results\n",
    "\n",
    "   To build the outputs, the list of collections and results is iterated over,\n",
    "   and the finalizer for each collection is called on its respective results.\n",
    "\n",
    "In pseudocode, this process looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(*collections, **kwargs):\n",
    "    # 1. Graph Merging & Optimization\n",
    "    # -------------------------------\n",
    "    if kwargs.pop('optimize_graph', True):\n",
    "        # If optimization is turned on, group the collections by\n",
    "        # optimization method, and apply each method only once to the merged\n",
    "        # sub-graphs.\n",
    "        optimization_groups = groupby_optimization_methods(collections)\n",
    "        graphs = []\n",
    "        for optimize_method, cols in optimization_groups:\n",
    "            # Merge the graphs and keys for the subset of collections that\n",
    "            # share this optimization method\n",
    "            sub_graph = merge_graphs([x.__dask_graph__() for x in cols])\n",
    "            sub_keys = [x.__dask_keys__() for x in cols]\n",
    "            # kwargs are forwarded to ``__dask_optimize__`` from compute\n",
    "            optimized_graph = optimize_method(sub_graph, sub_keys, **kwargs)\n",
    "            graphs.append(optimized_graph)\n",
    "        graph = merge_graphs(graphs)\n",
    "    else:\n",
    "        graph = merge_graphs([x.__dask_graph__() for x in collections])\n",
    "\n",
    "    # Keys are always the same\n",
    "    keys = [x.__dask_keys__() for x in collections]\n",
    "\n",
    "    # 2. Computation\n",
    "    # --------------\n",
    "    # Determine appropriate get function based on collections, global\n",
    "    # settings, and keyword arguments\n",
    "    get = determine_get_function(collections, **kwargs)\n",
    "    # Pass the merged graph, keys, and kwargs to ``get``\n",
    "    results = get(graph, keys, **kwargs)\n",
    "\n",
    "    # 3. Postcompute\n",
    "    # --------------\n",
    "    output = []\n",
    "    # Iterate over the results and collections\n",
    "    for res, collection in zip(results, collections):\n",
    "        finalize, extra_args = collection.__dask_postcompute__()\n",
    "        out = finalize(res, **extra_args)\n",
    "        output.append(out)\n",
    "\n",
    "    # `dask.compute` always returns tuples\n",
    "    return tuple(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-finish",
   "metadata": {},
   "source": [
    "### Persist\n",
    "\n",
    "Persist is very similar to ``compute``, except for how the return values are\n",
    "created. It too has three stages:\n",
    "\n",
    "1. **Graph Merging & Optimization**\n",
    "\n",
    "   Same as in ``compute``.\n",
    "\n",
    "2. **Computation**\n",
    "\n",
    "   Same as in ``compute``, except in the case of the distributed scheduler,\n",
    "   where the values in ``results`` are futures instead of values.\n",
    "\n",
    "3. **Postpersist**\n",
    "\n",
    "   Similar to ``__dask_postcompute__``, ``__dask_postpersist__`` is used to\n",
    "   rebuild values in a call to ``persist``. ``__dask_postpersist__`` returns\n",
    "   two things:\n",
    "\n",
    "   - A ``rebuild`` function, which takes in a persisted graph.  The keys of\n",
    "     this graph are the same as ``__dask_keys__`` for the corresponding\n",
    "     collection, and the values are computed results (for the single machine\n",
    "     scheduler) or futures (for the distributed scheduler).\n",
    "   - A tuple of extra arguments to pass to ``rebuild`` after the graph\n",
    "\n",
    "   To build the outputs of ``persist``, the list of collections and results is\n",
    "   iterated over, and the rebuilder for each collection is called on the graph\n",
    "   for its respective results.\n",
    "\n",
    "In pseudocode, this looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist(*collections, **kwargs):\n",
    "    # 1. Graph Merging & Optimization\n",
    "    # -------------------------------\n",
    "    # **Same as in compute**\n",
    "    graph = ...\n",
    "    keys = ...\n",
    "\n",
    "    # 2. Computation\n",
    "    # --------------\n",
    "    # **Same as in compute**\n",
    "    results = ...\n",
    "\n",
    "    # 3. Postpersist\n",
    "    # --------------\n",
    "    output = []\n",
    "    # Iterate over the results and collections\n",
    "    for res, collection in zip(results, collections):\n",
    "        # res has the same structure as keys\n",
    "        keys = collection.__dask_keys__()\n",
    "        # Get the computed graph for this collection.\n",
    "        # Here flatten converts a nested list into a single list\n",
    "        subgraph = {k: r for (k, r) in zip(flatten(keys), flatten(res))}\n",
    "\n",
    "        # Rebuild the output dask collection with the computed graph\n",
    "        rebuild, extra_args = collection.__dask_postpersist__()\n",
    "        out = rebuild(subgraph, *extra_args)\n",
    "\n",
    "        output.append(out)\n",
    "\n",
    "    # dask.persist always returns tuples\n",
    "    return tuple(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-finding",
   "metadata": {},
   "source": [
    "### Optimize\n",
    "\n",
    "The operation of ``optimize`` can be broken into two stages:\n",
    "\n",
    "1. **Graph Merging & Optimization**\n",
    "\n",
    "   Same as in ``compute``.\n",
    "\n",
    "2. **Rebuilding**\n",
    "\n",
    "   Similar to ``persist``, the ``rebuild`` function and arguments from\n",
    "   ``__dask_postpersist__`` are used to reconstruct equivalent collections from\n",
    "   the optimized graph.\n",
    "\n",
    "In pseudocode, this looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(*collections, **kwargs):\n",
    "    # 1. Graph Merging & Optimization\n",
    "    # -------------------------------\n",
    "    # **Same as in compute**\n",
    "    graph = ...\n",
    "\n",
    "    # 2. Rebuilding\n",
    "    # -------------\n",
    "    # Rebuild each dask collection using the same large optimized graph\n",
    "    output = []\n",
    "    for collection in collections:\n",
    "        rebuild, extra_args = collection.__dask_postpersist__()\n",
    "        out = rebuild(graph, *extra_args)\n",
    "        output.append(out)\n",
    "\n",
    "    # dask.optimize always returns tuples\n",
    "    return tuple(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-capacity",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "\n",
    "Visualize is the simplest of the 4 core functions. It only has two stages:\n",
    "\n",
    "1. **Graph Merging & Optimization**\n",
    "\n",
    "   Same as in ``compute``.\n",
    "\n",
    "2. **Graph Drawing**\n",
    "\n",
    "   The resulting merged graph is drawn using ``graphviz`` and outputs to the\n",
    "   specified file.\n",
    "\n",
    "In pseudocode, this looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(*collections, **kwargs):\n",
    "    # 1. Graph Merging & Optimization\n",
    "    # -------------------------------\n",
    "    # **Same as in compute**\n",
    "    graph = ...\n",
    "\n",
    "    # 2. Graph Drawing\n",
    "    # ----------------\n",
    "    # Draw the graph with graphviz's `dot` tool and return the result.\n",
    "    return dot_graph(graph, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-orbit",
   "metadata": {},
   "source": [
    "## Adding the Core Dask Methods to Your Class\n",
    "\n",
    "Defining the above interface will allow your object to used by the core Dask\n",
    "functions (``dask.compute``, ``dask.persist``, ``dask.visualize``, etc.). To\n",
    "add corresponding method versions of these, you can subclass from\n",
    "``dask.base.DaskMethodsMixin`` which adds implementations of ``compute``,\n",
    "``persist``, and ``visualize`` based on the interface above.\n",
    "\n",
    "---------\n",
    "\n",
    "## Example Dask Collection\n",
    "\n",
    "Here we create a Dask collection representing a tuple.  Every element in the\n",
    "tuple is represented as a task in the graph.  Note that this is for illustration\n",
    "purposes only - the same user experience could be done using normal tuples with\n",
    "elements of ``dask.delayed``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.base import DaskMethodsMixin, replace_name_in_key\n",
    "from dask.optimization import cull\n",
    "\n",
    "# We subclass from DaskMethodsMixin to add common dask methods to our\n",
    "# class. This is nice but not necessary for creating a dask collection.\n",
    "class Tuple(DaskMethodsMixin):\n",
    "    def __init__(self, dsk, keys):\n",
    "        # The init method takes in a dask graph and a set of keys to use\n",
    "        # as outputs.\n",
    "        self._dsk = dsk\n",
    "        self._keys = keys\n",
    "\n",
    "    def __dask_graph__(self):\n",
    "        return self._dsk\n",
    "\n",
    "    def __dask_keys__(self):\n",
    "        return self._keys\n",
    "\n",
    "    @staticmethod\n",
    "    def __dask_optimize__(dsk, keys, **kwargs):\n",
    "        # We cull unnecessary tasks here. Note that this isn't necessary,\n",
    "        # dask will do this automatically, this just shows one optimization\n",
    "        # you could do.\n",
    "        dsk2, _ = cull(dsk, keys)\n",
    "        return dsk2\n",
    "\n",
    "    # Use the threaded scheduler by default.\n",
    "    __dask_scheduler__ = staticmethod(dask.threaded.get)\n",
    "\n",
    "    def __dask_postcompute__(self):\n",
    "        # We want to return the results as a tuple, so our finalize\n",
    "        # function is `tuple`. There are no extra arguments, so we also\n",
    "        # return an empty tuple.\n",
    "        return tuple, ()\n",
    "\n",
    "    def __dask_postpersist__(self):\n",
    "        # We need to return a callable with the signature\n",
    "        # rebuild(dsk, *extra_args, rename: Mapping[str, str] = None)\n",
    "        return Tuple._rebuild, (self._keys,)\n",
    "\n",
    "    @staticmethod\n",
    "    def _rebuild(dsk, keys, *, rename=None):\n",
    "        if rename is not None:\n",
    "            keys = [replace_name_in_key(key, rename) for key in keys]\n",
    "        return Tuple(dsk, keys)\n",
    "\n",
    "    def __dask_tokenize__(self):\n",
    "        # For tokenize to work we want to return a value that fully\n",
    "        # represents this object. In this case it's the list of keys\n",
    "        # to be computed.\n",
    "        return self._keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-daily",
   "metadata": {},
   "source": [
    "Demonstrating this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add, mul\n",
    "\n",
    "# Define a dask graph\n",
    "dsk = {\"k0\": 1,\n",
    "       (\"x\", \"k1\"): 2,\n",
    "       (\"x\", 1): (add, \"k0\", (\"x\", \"k1\")),\n",
    "       (\"x\", 2): (mul, (\"x\", \"k1\"), 2),\n",
    "       (\"x\", 3): (add, (\"x\", \"k1\"), (\"x\", 1))}\n",
    "\n",
    "# The output keys for this graph.\n",
    "# The first element of each tuple must be the same across the whole collection;\n",
    "# the remainder are arbitrary, unique hashables\n",
    "keys = [(\"x\", \"k1\"), (\"x\", 1), (\"x\", 2), (\"x\", 3)]\n",
    "x = Tuple(dsk, keys)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-accounting",
   "metadata": {},
   "source": [
    "### Compute\n",
    "Turns Tuple into a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-quantity",
   "metadata": {},
   "source": [
    "### Persist\n",
    "Turns Tuple into a Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = x.persist()\n",
    "isinstance(x2, Tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-discharge",
   "metadata": {},
   "source": [
    "with each task already computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2.__dask_graph__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-memphis",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Try writing your own dask collection for different type of iterable. Some ideas are a dict, or a list.\n",
    "\n",
    "Optionally:\n",
    "\n",
    "- include a `classmethod` that takes in a standard python object and returns the dask collection.\n",
    "- write an `__len__` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-defeat",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "import dask\n",
    "from dask.base import DaskMethodsMixin, replace_name_in_key\n",
    "from dask.optimization import fuse\n",
    "\n",
    "\n",
    "class List(DaskMethodsMixin):\n",
    "    def __init__(self, dsk, keys):\n",
    "        self._dsk = dsk\n",
    "        self._keys = keys\n",
    "\n",
    "    def __dask_graph__(self):\n",
    "        return self._dsk\n",
    "\n",
    "    def __dask_keys__(self):\n",
    "        return self._keys\n",
    "\n",
    "    @staticmethod\n",
    "    def __dask_optimize__(dsk, keys, **kwargs):\n",
    "        dsk2, _ = fuse(dsk, keys)\n",
    "        return dsk2\n",
    "\n",
    "    # Use the threaded scheduler by default.\n",
    "    __dask_scheduler__ = staticmethod(dask.threaded.get)\n",
    "\n",
    "    def __dask_postcompute__(self):\n",
    "        return lambda args: list(chain(*args)), ()\n",
    "\n",
    "    def __dask_postpersist__(self):\n",
    "        return List._rebuild, (self._keys,)\n",
    "\n",
    "    @staticmethod\n",
    "    def _rebuild(dsk, keys, *, rename=None):\n",
    "        if rename is not None:\n",
    "            keys = [replace_name_in_key(key, rename) for key in keys]\n",
    "        return List(dsk, keys)\n",
    "\n",
    "    def __dask_tokenize__(self):\n",
    "        return self._keys\n",
    "\n",
    "    @classmethod\n",
    "    def from_list(cls, value, name, n_blocks=1):\n",
    "        dsk = {}\n",
    "        step = len(value) // n_blocks\n",
    "        i = 0\n",
    "        if step > 0:\n",
    "            while i * step < len(value) - step:\n",
    "                dsk[(name, i)] = value[i * step: (i + 1) * step]\n",
    "                i += 1\n",
    "        dsk[(name, i)] = value[i * step:]\n",
    "        return cls(dsk, list(dsk.keys()))\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(self._dsk[k]) for k in self._keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-print",
   "metadata": {},
   "source": [
    "## Checking if an object is a Dask collection\n",
    "\n",
    "To check if an object is a Dask collection, use\n",
    "``dask.base.is_dask_collection``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.base import is_dask_collection\n",
    "from dask import delayed\n",
    "\n",
    "x = delayed(sum)([1, 2, 3])\n",
    "assert is_dask_collection(x) is True\n",
    "assert is_dask_collection(1) is False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-classics",
   "metadata": {},
   "source": [
    "## Implementing Deterministic Hashing\n",
    "\n",
    "Dask implements its own deterministic hash function to generate keys based on\n",
    "the value of arguments.  This function is available as ``dask.base.tokenize``.\n",
    "Many common types already have implementations of ``tokenize``, which can be\n",
    "found in ``dask/base.py``.\n",
    "\n",
    "When creating your own custom classes, you may need to register a ``tokenize``\n",
    "implementation. There are two ways to do this:\n",
    "\n",
    "1. The ``__dask_tokenize__`` method\n",
    "\n",
    "   Where possible, it is recommended to define the ``__dask_tokenize__`` method.\n",
    "   This method takes no arguments and should return a value fully\n",
    "   representative of the object.\n",
    "\n",
    "2. Register a function with ``dask.base.normalize_token``\n",
    "\n",
    "   If defining a method on the class isn't possible or you need to\n",
    "   customize the tokenize function for a class whose super-class is\n",
    "   already registered (for example if you need to sub-class built-ins),\n",
    "   you can register a tokenize function with the ``normalize_token``\n",
    "   dispatch.  The function should have the same signature as described\n",
    "   above.\n",
    "\n",
    "In both cases the implementation should be the same, where only the location of the\n",
    "definition is different.\n",
    "\n",
    "> Both Dask collections and normal Python objects can have implementations of\n",
    "> ``tokenize`` using either of the methods described above.\n",
    "\n",
    "### Example\n",
    "\n",
    "Define a tokenize implementation using a method on the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.base import tokenize\n",
    "\n",
    "class Foo:\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def __dask_tokenize__(self):\n",
    "        # This tuple fully represents self\n",
    "        return (Foo, self.a, self.b)\n",
    "\n",
    "x = Foo(1, 2)\n",
    "assert tokenize(x) == tokenize(x)  # token is deterministic\n",
    "tokenize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-andrew",
   "metadata": {},
   "source": [
    "Register an implementation with normalize_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-vault",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.base import tokenize, normalize_token\n",
    "\n",
    "class Bar:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "@normalize_token.register(Bar)\n",
    "def tokenize_bar(x):\n",
    "    return (Bar, x.x, x.y)\n",
    "\n",
    "y = Bar(1, 2)\n",
    "assert tokenize(y) == tokenize(y)\n",
    "assert tokenize(y) != tokenize(x)  # tokens for different objects aren't equal\n",
    "tokenize(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-restoration",
   "metadata": {},
   "source": [
    "For more examples, see ``dask/base.py`` or any of the built-in Dask collections."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
